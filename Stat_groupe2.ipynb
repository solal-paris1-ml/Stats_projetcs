{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Statistical Learning Project\n",
    "\n",
    "We need to categorize th problem. With the data we have, in this particular contexte, we want to classify in a book is rather a fiction or not.\n",
    "\n",
    "Here we have the analysing task. We don't need to pre-process (note that much I mean)\n",
    "\n",
    "Later, it will be interesting to transform the raw data with futher features (or transformation on the variables).\n",
    "\n",
    "We also need to adress all if this points:\n",
    "\n",
    "- The accuracy of the model. (we need to be careful with this)\n",
    "- The interpretability of the model.\n",
    "- The complexity of the model.\n",
    "- The scalability of the model.\n",
    "- How long does it take to build, train, and test the model?\n",
    "- How long does it take to make predictions using the model?\n",
    "- Does the model meet the business goal ?\n",
    "\n",
    "And also: Optimize hyperparameters with (grid search, random search or Bayesian optimisation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"IMPORT OF PCK\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import scipy.signal as signal\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.core.display import display, HTML #display(HTML(data.to_html())) to show all variables\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"DIRECTORY\"\"\"\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1/ Import of data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_folder = Path(r\"C:\\Users\\ville\\PycharmProjects\\stat_projetcs\\git_project_stat\\Stats_projetcs\\Appr_Stat\\data\\Books_Dataset_20210108\\archive\")\n",
    "bestsellers = data_folder / \"bestsellers_with_categories.csv\"\n",
    "dta = pd.read_csv(bestsellers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2/ Data Wrangling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Corrections for modelisation steps\n",
    "\n",
    "dta[\"Genre\"] = dta.Genre.replace(to_replace=['Fiction', 'Non Fiction'], value=[0, 1])\n",
    "dta = dta.rename(columns={\"User Rating\": \"Rating\"})\n",
    "dta.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(dta.head(5))\n",
    "print(dta.tail(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>\n",
    "<b>\n",
    "\n",
    "We can see from the tail of our dataset that there are the same book, a multiple time.\n",
    "\n",
    "The only difference is the year, in term of features. We can imagine that it represent the year when the book pop_up or the year when it becomes a bestsellers.\n",
    "After a quick verification on internet, we can see that it represents: The year when the book was past of bestsellings.\n",
    "Lets create a feature representing the time the book had been a bestsellings.\n",
    "\n",
    "In the next steps, we will treat this problematic."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## -- Cleaning of Name column\n",
    "Due to case sensitivity, the same book, with a different writing, can be misinterpreted to a different one.\n",
    "We can correct this errors of case sensitivity by putting all the name in upper case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dta[\"Name\"] = dta[\"Name\"].str.upper()\n",
    "#problem solved"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## -- Cleaning of Author column\n",
    "\n",
    "We identified some difference for the reference of some authors, such as J.K Rowling. To fix this error, we smooth out the author name to J.K Rowling (without space between \".\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#To correct the space problem for few authors\n",
    "for index, value in dta[\"Author\"].items():\n",
    "    if value.startswith(\"J.\"):\n",
    "        change = value.replace(\" \",\"\")\n",
    "        dta.loc[index, \"Author\"] = change\n",
    "    else:\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creation of a new column which corresponds to the sum of year being best-sellings in total for the previous ten years.\n",
    "Next we show the graph corresponding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dta[\"sum_best\"]=\"\"\n",
    "dta[\"sum_best\"] = dta.groupby(\"Name\").transform('count')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "ax = sns.countplot(data=dta, x=dta.Name.value_counts(), palette=\"Set2\");\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), 'd'),\n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                   ha = 'center', va = 'center',\n",
    "                   xytext = (0, 9),\n",
    "                   textcoords = 'offset points')\n",
    "\n",
    "ax.set(xlabel=\"Appearances\", ylabel=\"Count\");\n",
    "ax.set_title(\"Times a Title Shows up on Top 50 Charts (2009-2019)\");\n",
    "plt.show();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dta_repeat = pd.DataFrame([x, y] for x, y in dta.Author.value_counts().iteritems() if (y > 1))\n",
    "dta_repeat.columns = ['Author','Top 50 Appearances']\n",
    "dta_repeat.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% #steps for futher analysis\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "ax = sns.countplot(data=dta_repeat, x=\"Top 50 Appearances\", palette=\"Set2\");\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate('{:.1%}'.format(p.get_height()/len(dta_repeat)),\n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                   ha = 'center', va = 'center',\n",
    "                   xytext = (0, 9),\n",
    "                   textcoords = 'offset points')\n",
    "\n",
    "ax.set(xlabel=\"Appearances\", ylabel=\"Count\");\n",
    "ax.set_title(\"Repeat Top 50 Appearances\");\n",
    "plt.show();\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Looking for redundancy in the authors\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "ax = sns.countplot(data=dta, x=dta.Author.value_counts(), palette=\"Set2\")\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), 'd'),\n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                   ha = 'center', va = 'center',\n",
    "                   xytext = (0, 9),\n",
    "                   textcoords = 'offset points')\n",
    "\n",
    "ax.set(xlabel=\"Appearances\", ylabel=\"Number of Authors\")\n",
    "ax.set_title(\"Total Appearances in the Top 50 Charts\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## -- Basic statistics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dta.describe().transpose()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## -- Look for correlation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dta.shape\n",
    "\n",
    "numerical = list(set(dta.columns) -\n",
    "                 set(['Name','Author']))\n",
    "corr_matrix = dta[numerical].corr()\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True);\n",
    "plt.xticks(rotation=45);\n",
    "\n",
    "print(corr_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## -- Add of a weighted review column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Give an information on the dataset with the cosidered \"best movies\", with minimum 3000 review\n",
    "data_order = dta.groupby(['Name', 'Author', 'Genre'], as_index=False)[['Rating', 'Reviews']].mean()\n",
    "data_order = data_order[data_order['Reviews']>3000]\n",
    "data_order = data_order.sort_values('Rating', ascending=False).head(20)\n",
    "data_order\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use of weighted mean to attribute a review/rating which corresponds more to the reality field\n",
    "\n",
    "with W_mean = ( ( v / m+v) * R) (( m / m+v)*C)\n",
    "\n",
    "m = number of rating\n",
    "\n",
    "C = mean Review\n",
    "\n",
    "v= number of review\n",
    "\n",
    "R = average numb of rating\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Weighted rating\n",
    "m = min(dta['Reviews'])\n",
    "C = dta['Rating'].mean()\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['Reviews']\n",
    "    R = x['Rating']\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dta['Weighted Rating'] = dta.apply(weighted_rating, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dta.groupby(['Name','Author','Genre'], as_index=False)[['Rating', 'Reviews', 'Weighted Rating']].mean().sort_values(by='Weighted Rating', ascending=False).head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dta.shape\n",
    "\n",
    "numerical = list(set(dta.columns) -\n",
    "                 set(['Name','Author']))\n",
    "corr_matrix = dta[numerical].corr()\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True);\n",
    "plt.xticks(rotation=45);\n",
    "\n",
    "print(corr_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dta = dta.drop(\"Rating\", axis=1)\n",
    "dta.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## -- Step Outlier detection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualisation des données pour la detection d'outlier\n",
    "### Diagrammes à moustache des variables quantitatives\n",
    "\n",
    "\n",
    "#Boxplot pour les variables quatitatives"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dta_quanti = dta[dta.columns.difference([\"Name\", \"Author\"])]\n",
    "c = 'steelblue'\n",
    "blue_dict =  {'patch_artist': True,\n",
    "             'boxprops': dict(color=c, facecolor='w'),\n",
    "             'capprops': dict(color=c),\n",
    "             'flierprops': dict(color=c, markeredgecolor=c,marker=\".\"),\n",
    "             'medianprops': dict(color=c),\n",
    "             'whiskerprops': dict(color=c)}\n",
    "\n",
    "plt.close()\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in np.arange(start=0,stop=dta_quanti.shape[1]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.boxplot(dta_quanti.iloc[:,i],**blue_dict)\n",
    "    plt.xlabel(dta_quanti.columns[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## -- Step Bivariate analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#With genre 0 = Fiction, genre 1 = Non fiction\n",
    "sns.pairplot(dta, hue='Genre',vars=['Reviews', 'Price', 'Year',\n",
    "       'sum_best', 'Weighted Rating'],markers='o')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pairgrid = sns.PairGrid(dta,vars=['Reviews', 'Price', 'Year',\n",
    "       'sum_best', 'Weighted Rating'], size = 4)\n",
    "pairgrid = pairgrid.map_upper(plt.scatter, color = 'red', edgecolor = 'black', alpha = 0.2, s = 10)\n",
    "pairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'red',alpha = 0.2, edgecolor = 'k')# Map a density plot to the lower triangle\n",
    "pairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno,\n",
    "                              shade = False, shade_lowest = False, alpha = 1.0, n_levels = 10)\n",
    "pairgrid.add_legend()\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=0.6, top=0.6, wspace=0.2, hspace=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Modelisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## -- List of model we will use for our binairy classification\n",
    "\n",
    "Note that we have to be careful with the rzegulaization step with the algorithm based on euclidian distance.\n",
    "\n",
    "- Logistic Regression.\n",
    "- k-Nearest Neighbors.\n",
    "- Decision Trees.\n",
    "- Support Vector Machine.\n",
    "- Naive Bayes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#impot of pck for modelisation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from warnings import simplefilter\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# definition of train: + train test slpit of data\n",
    "\n",
    "seed = 7\n",
    "validation_size = 0.5\n",
    "scoring=\"accuracy\"\n",
    "#shuffle of the data\n",
    "model_dta = dta_quanti.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = model_dta.loc[:, model_dta.columns != 'Genre'].to_numpy().astype(\"int8\")\n",
    "y = model_dta.loc[:,\"Genre\"].to_numpy().astype(\"int8\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=validation_size, random_state=seed)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note for now we execute a train test split of 80/20, in the next steps, we will rafinate this part."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('RDF', RandomForestClassifier(n_estimators=100)))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('SVC', SVC(gamma='auto')))\n",
    "models.append(('GauNB', GaussianNB()))\n",
    "\n",
    "results=[]\n",
    "names=[]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Lname=[models[0][0], models[1][0], models[2][0], models[3][0], models[4][0]]\n",
    "Lmodel=[models[0][1], models[1][1], models[2][1], models[3][1], models[4][1]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prediction(N,M):\n",
    "#for name,model in models:\n",
    "    from warnings import simplefilter\n",
    "    simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "    kfold=model_selection.KFold(n_splits=10,random_state=seed)\n",
    "    cv_results=model_selection.cross_val_score(M,X_train,y_train,cv=10,scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(N)\n",
    "    msg = \"%s: %f (%f)\" % (N, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Lgt_accuracy = prediction(models[0][0],models[0][1])\n",
    "Rdf_accuracy =  prediction(models[1][0],models[1][1])\n",
    "KNN_accuracy=   prediction(models[2][0],models[2][1])\n",
    "SVC_accuracy =  prediction(models[3][0],models[3][1])\n",
    "GauNB_accuracy=   prediction(models[4][0],models[4][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameters optimization\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}